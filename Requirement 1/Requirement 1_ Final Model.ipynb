{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"iouZeeURraFH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn import preprocessing\n","\n","import time\n","import numpy as np\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')"],"metadata":{"id":"6RBM3524_ezT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import stop words, data and do some pre processing\n","stop_words = set(stopwords.words('english'))\n","with open('/content/stopwords.txt') as f:\n","    for line in f:\n","        stop_words.add(line[:-1])\n","stop_words = list(stop_words)\n","#Read the csv file and change the encoding, remove tags,lower them\n","df_train = pd.read_csv('/content/drive/MyDrive/bigdata2023-exercise1-classification/train.csv', encoding='utf-8')\n","df_train['Title'] = df_train['Title'].str.encode('ascii', 'ignore').str.decode('ascii').str.lower().str.replace('<br />','')\n","df_train['Content'] = df_train['Content'].str.encode('ascii', 'ignore').str.decode('ascii').str.lower().str.replace('<br />','')\n","df_train['Label'] = df_train['Label'].str.encode('ascii', 'ignore').str.decode('ascii').str.lower().str.replace('<br />','')\n","\n","#make a new column as a combination of title & content \n","df_train['Combined']  = 3*(df_train['Title'] + ' ')  + df_train['Content'] "],"metadata":{"id":"AvGYgErL_kAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#initilizations\n","le = preprocessing.LabelEncoder() # encode labels with a value between 0 and n_classes-1 \n","y = le.fit_transform(df_train['Label'])\n","clf = LinearSVC(random_state=42, tol=1e-5) #implement Linear Support Vector Machine"],"metadata":{"id":"Zw1ERkTu_xHE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create vectorizer\n","vectorizer = TfidfVectorizer(stop_words=stop_words) #converts a collection of raw documents into a matrix of TF-IDF features and removing stopwords\n","X = vectorizer.fit_transform(df_train['Combined'])"],"metadata":{"id":"YD4VuqqK_w6b"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gjzPsRiXe4Wb","executionInfo":{"status":"ok","timestamp":1675605415050,"user_tz":-120,"elapsed":86177,"user":{"displayName":"Mariangela Pollali","userId":"17063638695940791853"}},"outputId":"837a5ca9-2e91-4b92-f6bd-3bfd9ee82f6c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['mon'] not in stop_words.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Running on test set...\n","Training Linear SVM...\n","Finished training...\n"]}],"source":["#Read the csv file and change the encoding\n","df_test = pd.read_csv('/content/drive/MyDrive/bigdata2023-exercise1-classification/test_without_labels.csv', encoding='utf-8')\n","df_test['Title'] = df_test['Title'].str.encode('ascii', 'ignore').str.decode('ascii').str.lower().str.replace('<br />','')\n","df_test['Content'] = df_test['Content'].str.encode('ascii', 'ignore').str.decode('ascii').str.lower().str.replace('<br />','')\n","df_test['Combined']  = 3*(df_test['Title'] + ' ')  + df_test['Content']"]},{"cell_type":"code","source":["#Training Linear SVM\n","clf.fit(X, y)"],"metadata":{"id":"VRBP0_HzAV1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = vectorizer.transform(df_test['Combined'])\n","#predict\n","predictions = clf.predict(X)\n","predictions = le.inverse_transform(predictions)"],"metadata":{"id":"wGW5MKhMAk1Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = pd.DataFrame({'Id':df_test['Id'],'Predicted':predictions})\n","result.to_csv('testSet_categories.csv', sep=',', index=False)"],"metadata":{"id":"DCyKSOM5_3ib"},"execution_count":null,"outputs":[]}]}