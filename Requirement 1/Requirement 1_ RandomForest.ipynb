{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPc/WY+zxVkv01V+1tGhwO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R0iFL5Wvx7ra","executionInfo":{"status":"ok","timestamp":1675620512516,"user_tz":-120,"elapsed":22057,"user":{"displayName":"Mariangela Pollali","userId":"17063638695940791853"}},"outputId":"83f06827-93ff-49a6-fb6c-52bfbdc010f6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn import preprocessing\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","  \n","\n","import time\n","import numpy as np\n","from nltk.corpus import stopwords"],"metadata":{"id":"syJEoGK3BmdL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675620521697,"user_tz":-120,"elapsed":9197,"user":{"displayName":"Mariangela Pollali","userId":"17063638695940791853"}},"outputId":"820384d4-ea1f-4f75-e956-631935710484"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["#import stop words, data and do some pre processing\n","stop_words = set(stopwords.words('english'))\n","with open('/content/stopwords.txt') as f:\n","    for line in f:\n","        stop_words.add(line[:-1])\n","stop_words = list(stop_words)\n","#Read the csv file and change the encoding, remove tags,lower them\n","df_train = pd.read_csv('/content/drive/MyDrive/bigdata2023-exercise1-classification/train.csv', encoding='utf-8')\n","df_train['Title'] = df_train['Title'].str.encode('ascii', 'ignore').str.decode('ascii').str.lower().str.replace('<br />','')\n","df_train['Content'] = df_train['Content'].str.encode('ascii', 'ignore').str.decode('ascii').str.lower().str.replace('<br />','')\n","df_train['Label'] = df_train['Label'].str.encode('ascii', 'ignore').str.decode('ascii').str.lower().str.replace('<br />','')\n","\n","#make a new column as a combination of title & content \n","df_train['Combined']  = 3*(df_train['Title'] + ' ')  + df_train['Content']"],"metadata":{"id":"r3aJoNilBoZ_","executionInfo":{"status":"ok","timestamp":1675620536191,"user_tz":-120,"elapsed":14508,"user":{"displayName":"Mariangela Pollali","userId":"17063638695940791853"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#initializations\n","le = preprocessing.LabelEncoder()\n","y = le.fit_transform(df_train['Label'])\n","clf = RandomForestClassifier(n_estimators=100, max_depth=18, random_state=42)"],"metadata":{"id":"d6_05ilqBsHU","executionInfo":{"status":"ok","timestamp":1675620536192,"user_tz":-120,"elapsed":33,"user":{"displayName":"Mariangela Pollali","userId":"17063638695940791853"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["total_time = time.time()\n","vectorizer = TfidfVectorizer(stop_words=stop_words)\n","X = vectorizer.fit_transform(df_train['Combined'])\n","\n","kfold_time = time.time()\n","kf = KFold(n_splits=5)\n","accuracy = 0\n","precision = 0\n","recall = 0\n","fmeasure = 0\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    clf.fit(X_train, y_train)\n","    predictions = clf.predict(X_test)\n","    \n","    accuracy += accuracy_score(y_test, predictions)\n","    precision += precision_score(y_test, predictions, average='macro')\n","    recall += recall_score(y_test, predictions, average='macro')\n","    fmeasure += f1_score(y_test, predictions, average='macro')\n","\n","accuracy /= 5\n","precision /= 5\n","recall /= 5\n","fmeasure /= 5\n","\n","print('accuracy = {}, precision = {}, recall = {}, f1-measure = {}'.format(round(accuracy, 4), round(precision,4), round(recall,4), round(fmeasure,4)))\n","print('5-fold time: {} s'.format(time.time() - kfold_time))\n","print('Total for RandomForestClassifier: {} s'.format(time.time() - total_time))\n","\n","total_time = time.time()"],"metadata":{"id":"aA8qcO3NBxFn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675621045414,"user_tz":-120,"elapsed":509254,"user":{"displayName":"Mariangela Pollali","userId":"17063638695940791853"}},"outputId":"0ef4f26a-fc48-449b-8011-dc9915ed58e4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['mon'] not in stop_words.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["accuracy = 0.7569, precision = 0.866, recall = 0.6749, f1-measure = 0.7228\n","5-fold time: 467.7227358818054 s\n","Total for RandomForestClassifier: 509.08939146995544 s\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5bJ6YOIkxX-5","executionInfo":{"status":"ok","timestamp":1675605149587,"user_tz":-120,"elapsed":773664,"user":{"displayName":"Mariangela Pollali","userId":"17063638695940791853"}},"outputId":"688b7f5c-52fb-495c-e115-c2d5d055f1e2"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['mon'] not in stop_words.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Starting 5-fold for BOW\n","accuracy = 0.7558, precision = 0.8656, recall = 0.6746, f1-measure = 0.7225\n","5-fold time: 428.73845505714417 s\n","Total for BOW: 469.54180216789246 s\n","Starting 5-fold for SVD\n","accuracy = 0.9351, precision = 0.9305, recall = 0.9243, f1-measure = 0.9273\n","5-fold time: 282.126202583313 s\n","Total for SVD: 295.05093145370483 s\n"]}],"source":["#with SVD\n","svd = TruncatedSVD(n_components=20, random_state=42) \n","X = svd.fit_transform(X)\n","\n","print('Starting 5-fold for SVD')\n","kfold_time = time.time()\n","kf = KFold(n_splits=5)\n","accuracy = 0\n","precision = 0\n","recall = 0\n","fmeasure = 0\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    clf.fit(X_train, y_train)\n","    predictions = clf.predict(X_test)\n","    \n","    accuracy += accuracy_score(y_test, predictions)\n","    precision += precision_score(y_test, predictions, average='macro')\n","    recall += recall_score(y_test, predictions, average='macro')\n","    fmeasure += f1_score(y_test, predictions, average='macro')\n","\n","accuracy /= 5\n","precision /= 5\n","recall /= 5\n","fmeasure /= 5\n","\n","print('accuracy = {}, precision = {}, recall = {}, f1-measure = {}'.format(round(accuracy, 4), round(precision,4), round(recall,4), round(fmeasure,4)))\n","print('5-fold time: {} s'.format(time.time() - kfold_time))\n","print('Total for RandomForestClassifier with SVD: {} s'.format(time.time() - total_time))\n","\n","\n"]}]}